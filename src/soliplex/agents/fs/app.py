import hashlib
import json
import logging
import mimetypes
import pathlib
from pathlib import Path

import aiofiles
import aiofiles.os as aos

from soliplex.agents import ValidationError
from soliplex.agents import client

logger = logging.getLogger(__name__)
IGNORE_EXTENSIONS = ["png", "jpg", "md", "txt", "json", "csv", "zip"]
MIME_OVERRIDES = {
    "application/vnd.openxmlformats-officedocument.wordprocessingml.document": ".docx",  # noqa: E501
    "application/vnd.openxmlformats-officedocument.presentationml.presentation": ".pptx",  # noqa: E501
    "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet": ".xlsx",  # noqa: E501
}


async def validate_config(path: str):
    """
    Validate a configuration file and print out validation results

    Args:
        path: The path to the configuration file to validate

    Returns:
        None
    """
    config = await read_config(path)
    validate = check_config(config)
    invalid = [row for row in validate if "valid" in row and not row["valid"]]
    print(f"Validation  for {path}")
    print(f"Total files: {len(config)}")
    if invalid:
        print(f"Found {len(invalid)} Invalid files:")
        for row in invalid:
            print(row["path"], row["reason"], row["metadata"]["content-type"])


async def read_config(config_path: str) -> list[dict]:
    logger.debug(f"Reading config from {config_path}")
    async with aiofiles.open(config_path) as f:
        config = json.loads(await f.read())
        ret = config
        if isinstance(config, list):
            ret = config
        elif isinstance(config, dict) and "data" in config.keys():
            ret = config["data"]
        else:
            raise ValidationError(config_path)

        ret = sorted(ret, key=lambda x: int(x["metadata"]["size"]))

        return ret


async def build_config(source_dir) -> list[dict]:
    paths = await recursive_listdir(Path(source_dir))
    config = []
    for path in paths:
        ext = path.name.split(".")[-1]
        if ext in IGNORE_EXTENSIONS:
            continue
        adj_path = path.relative_to(Path(source_dir))
        mime_type = mimetypes.guess_type(str(adj_path))[0]
        if mime_type is None:
            mime_type = MIME_OVERRIDES.get(mime_type, "application/octet-stream")
        rec = {
            "path": str(adj_path),
            "sha256": hashlib.sha256(path.read_bytes()).hexdigest(),
            "metadata": {
                "size": path.stat().st_size,
                "content-type": mime_type,
            },
        }
        config.append(rec)
    return config


async def recursive_listdir(file_dir: pathlib.Path):
    file_paths = []
    ls = await aos.listdir(file_dir)
    for entry in ls:
        ed = file_dir / entry
        isdir = await aos.path.isdir(ed)
        if isdir:
            ext = await recursive_listdir(ed)
            file_paths.extend(ext)
        else:
            file_paths.append(ed)
    return file_paths


def check_config(config: list[dict], start: int = 0, end: int = None) -> list[dict]:
    for row in config:
        path = row["path"]
        ext = path.split(".")[-1]
        row["valid"] = True
        if "metadata" in row and "content-type" in row["metadata"]:
            content_type = row["metadata"]["content-type"]
            if content_type in [
                "application/zip",
                "application/x-zip-compressed",
                "application/octet-stream",
                "application/x-rar-compressed",
                "application/x-7z-compressed",
            ]:
                row["valid"] = False
                row["reason"] = "Unsupported content type"
        else:
            row["valid"] = False
            row["reason"] = "No content type"

        if len(ext) > 4:
            row["valid"] = False
            row["reason"] = f"Unsupported file extension {ext}"
    return config


async def load_inventory(
    path: str,
    source: str,
    start: int = 0,
    end: int = None,
    resume_batch: int = None,
    skip_invalid: bool = False,
    workflow_definition_id: str | None = None,
    start_workflows: bool = True,
    param_set_id: str | None = None,
    priority: int = 0,
):
    config = await read_config(path)
    if skip_invalid:
        filtered = check_config(config)
        config = [x for x in filtered if x["valid"]]

    logger.info(f"found {len(config)} files in {path}")
    data_path = Path(path).parent
    logger.info(f"data_path={data_path}")
    to_process = await client.check_status(config, source)
    if end is None:
        end = len(config)
        to_process = to_process[start:end]

    if resume_batch:
        batch_id = resume_batch
    else:
        batch_id = await client.create_batch(
            source,
            source,
        )
    errors = []
    for row in to_process:
        meta = row["metadata"].copy()
        for k in [
            "path",
            "sha256",
            "size",
            "source",
            "batch_id",
            "source_uri",
        ]:
            if k in meta:
                del meta[k]
        logger.info(f"starting ingest for {row['path']}")
        mime_type = None
        if "metadata" in row and "content-type" in row["metadata"]:
            mime_type = row["metadata"]["content-type"]
        res = await do_ingest(
            data_path,
            row["path"],
            meta,
            source,
            batch_id,
            mime_type,
        )
        if "error" in res:
            logger.error(f"Error ingesting {row['path']}: {res['error']}")
            res["uri"] = row["path"]
            res["source"] = source
            res["resumed_batch"] = resume_batch
            res["batch_id"] = batch_id
            errors.append(res)
    if len(errors) > 0 and start_workflows:
        ret = await client.do_start_workflows(
            batch_id,
            workflow_definition_id,
            param_set_id,
            priority,
        )
        return ret
    else:
        return errors


async def do_ingest(
    data_path: Path,
    uri: str,
    meta: dict[str, str],
    source: str,
    batch_id: int,
    mime_type: str,
):
    logger.info(f"path={data_path / uri}")
    load_path = uri
    if not uri.startswith("/"):
        load_path = data_path / uri
    async with aiofiles.open(load_path, "rb") as f:
        doc_body = await f.read()

    return await client.do_ingest(
        doc_body,
        uri,
        meta,
        source,
        batch_id,
        mime_type,
    )


async def status_report(config_path: str, source: str, detail: bool = False):
    print(f"checking status for {config_path} source={source} ")
    config = await read_config(config_path)
    to_process = await client.check_status(config, source)
    print(f"Files to process: {len(to_process)}")
    print(f"Total files: {len(config)}")
    if detail and len(to_process) > 0:
        for row in to_process:
            print(row)
